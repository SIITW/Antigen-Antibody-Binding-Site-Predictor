{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training PIsToN\n",
    "\n",
    "This notebook is a supplemental material to the paper titled \"PIsToN: Evaluating Protein Binding Interfaces with Transformer Networks.\"\n",
    "\n",
    "The notebook provides an example on how to train the PIsToN model.\n",
    " \n",
    "If you are using this code, please cite the following paper:\n",
    "\n",
    "Stebliankin V, Shirali A, Baral P, Chapagain P, Narasimhan G. PIsToN: Evaluating Protein Binding Interfaces with Transformer Networks. bioRxiv. 2023 Jan 4:2023-01.\n",
    "\n",
    "---\n",
    "Vitalii Stebliankin (vsteb002@fiu.edu) <br>\n",
    "Bioinformatics Research Group (BioRG) <br>\n",
    "Florida International University\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T03:54:02.530637500Z",
     "start_time": "2023-12-30T03:53:36.378483800Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_368066/2141752603.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../../src'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPDB_complex_training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_val\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_processed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/Data1/23wxy/piston/utils/trainer.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchsummaryX\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msummary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mray\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtune\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#### Local import\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from utils.dataset import PDB_complex_training\n",
    "from utils.trainer import fit, evaluate_val\n",
    "from utils.utils import get_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.541617900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Description of features:\n",
    "    # 0 - Shape index | p1\n",
    "    # 1 - Distance depended curvature | p1\n",
    "    # 2 - Hydrogen bond potential | p1\n",
    "    # 3 - Charge | p1\n",
    "    # 4 - Hydrophobicity | p1\n",
    "    # 5 - Shape index | p1\n",
    "    # 6 - Distance depended curvature | p1\n",
    "    # 7 - Hydrogen bond potential | p1\n",
    "    # 8 - Charge | p1\n",
    "    # 9 - Hydrophobicity | p1\n",
    "    # 10 - Distance between atoms | p1 and p2\n",
    "    # 11 - Relative ASA | p1\n",
    "    # 12 - Relative ASA | p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.593369700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants and directories\n",
    "MARGIN=0\n",
    "TEMP=0.5\n",
    "DIM=16\n",
    "\n",
    "DATA_DIR = os.getcwd()+'./data_preparation/'\n",
    "\n",
    "TRAIN_LIST_FILE = '../data/lists/training.txt'\n",
    "VAL_LIST_FILE = '../data/lists/final_val.txt'\n",
    "\n",
    "PATIENCE=10 # number of consequitive times when model don't improve before early stopping\n",
    "SEED_ID = 7272\n",
    "BATCH_SIZE=1\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "MODEL_NAME=f'PIsToN_multiAttn_contrast'\n",
    "MODEL_DIR=f'./savedModels/{MODEL_NAME}'\n",
    "IMG_SIZE=32\n",
    "\n",
    "# Select only original MaSIF features\n",
    "FEATURES_SUBSET = list(range(13)) # Use all features\n",
    "N_FEATURES=len(FEATURES_SUBSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.593369700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = {}\n",
    "\n",
    "config['dirs'] = {}\n",
    "config['dirs']['data_prepare'] = DATA_DIR\n",
    "config['dirs']['grid'] = config['dirs']['data_prepare'] + '07-grid/'\n",
    "config['dirs']['docked'] = config['dirs']['data_prepare'] + 'docked/'\n",
    "config['dirs']['tmp'] = '/aul/homes/vsteb002/tmp'\n",
    "\n",
    "config['ppi_const'] = {}\n",
    "config['ppi_const']['patch_r'] = 16 # 16\n",
    "\n",
    "os.environ[\"TMP\"] = config['dirs']['tmp']\n",
    "os.environ[\"TMPDIR\"] = config['dirs']['tmp']\n",
    "os.environ[\"TEMP\"] = config['dirs']['tmp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of processed complexes\n",
    "\n",
    "In some cases, a complex can't be pre-processed due to the memory overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.593369700Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import get_processed\n",
    "\n",
    "train_list = [x.strip('\\n') for x in open(TRAIN_LIST_FILE, 'r').readlines()]\n",
    "val_list = [x.strip('\\n') for x in open(VAL_LIST_FILE, 'r').readlines()]\n",
    "\n",
    "train_list_updated = get_processed(train_list, config)\n",
    "val_list_updated = get_processed(val_list, config)\n",
    "\n",
    "print(f\"{len(train_list_updated)}/{len(train_list)} training complexes were processed for 12A\")\n",
    "print(f\"{len(val_list_updated)}/{len(val_list)} validation complexes were processed for 12A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn mean and standard deviation for standard scaling for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.593369700Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read all positive complexes\n",
    "grid_native_list = []\n",
    "for ppi in train_list:\n",
    "    grid_path = f\"{config['dirs']['grid']}/{ppi}.npy\"\n",
    "    if os.path.exists(grid_path):\n",
    "        grid_native_list.append(np.load(grid_path, allow_pickle=True))\n",
    "\n",
    "print(f\"Loaded {len(grid_native_list)} native complexes\")\n",
    "all_grid = np.stack(grid_native_list, axis=0)\n",
    "\n",
    "radius = config['ppi_const']['patch_r']\n",
    "\n",
    "\n",
    "std_array = np.ones(N_FEATURES)\n",
    "mean_array = np.zeros(N_FEATURES)\n",
    "\n",
    "feature_pairs = {\n",
    "    'shape_index': (0, 5),\n",
    "    'ddc': (1, 6),\n",
    "    'electrostatics': (2, 7),\n",
    "    'charge': (3, 8),\n",
    "    'hydrophobicity': (4, 9),\n",
    "    'patch_dist':(10,),\n",
    "    'SASA': (11,12)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.593369700Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature in feature_pairs.keys():\n",
    "    print(f\"Obtaining pixel values for {feature}\")\n",
    "    pixel_values = []\n",
    "    for feature_i in feature_pairs[feature]:\n",
    "        print(f\"Index {feature_i}\")\n",
    "        for image_i in tqdm(range(all_grid.shape[0])):\n",
    "            for row_i in range(all_grid.shape[1]):\n",
    "                for column_i in range(all_grid.shape[2]):\n",
    "                    # Check if coordinates are within the radius\n",
    "                    x = column_i - radius\n",
    "                    y = radius - row_i\n",
    "                    if x**2 + y**2 < radius**2:\n",
    "                        pixel_values.append(all_grid[image_i][row_i][column_i][feature_i])\n",
    "\n",
    "    mean_value = np.mean(pixel_values)\n",
    "    std_value = np.std(pixel_values)\n",
    "    print(f\"Feature {feature}; Mean: {mean_value}; std: {std_value}\")\n",
    "    for feature_i in feature_pairs[feature]:\n",
    "        mean_array[feature_i] = mean_value\n",
    "        std_array[feature_i] = std_value\n",
    "        \n",
    "print(\"Mean array:\")\n",
    "print(list(mean_array))\n",
    "print(\"\")\n",
    "print(\"Standard deviation array:\")\n",
    "print(list(std_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn mean and std of energy terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_energies(energies_path, assign_zeros=False):\n",
    "    # energies_path - path to the energy file computed by firedock\n",
    "    # assign_zeros - if set True and energies file is empty, assign zeros to all energies (default is False)\n",
    "    \n",
    "    \"\"\"\n",
    "    :param ppi:\n",
    "    :return: numpy array of energy terms:\n",
    "        (0) - indx\n",
    "        (1) - Lrmsd     - ligand rmsd of the final position, after the rigid-body optimization.\n",
    "        (2) -Irmsd     - interface rmsd of the final position, after the rigid-body optimization.\n",
    "        (3) - st_Lrmsd  - initial ligand rmsd.\n",
    "        (4) - st_Irmsd  - initial ligand rmsd.\n",
    "    0 - (5) - glob      - global score of the candidate, which is linear combination of the terms described bellow. To rank the candidates, you should sort the rows by this column in ascending order.\n",
    "    1 - (6) - aVdW      - attractive van der Waals\n",
    "    2 - (7) - rVdW      - repulsive van der Waals\n",
    "    3 - (8) - ACE       - Atomic Contact Energy\n",
    "    4 - (9) - inside    - \"Insideness\" measure, which reflects the concavity of the interface.\n",
    "    5 - (10) - aElec     - short-range attractive electrostatic term\n",
    "    6 - (11) - rElec     - short-range repulsive electrostatic term\n",
    "    7 - (12) - laElec    - long-range attractive electrostatic term\n",
    "    8 - (13) - lrElec    - long-range repulsive electrostatic term\n",
    "    9 - (14) - hb        - hydrogen and disulfide bonding\n",
    "    10 - (15) - piS\t  - pi-stacking interactions\n",
    "    11 - (16) - catpiS\t  - cation-pi interactions\n",
    "    12 - (17) - aliph\t  - aliphatic interactions\n",
    "         (18) - prob      - rotamer probability\n",
    "\n",
    "    \"\"\"    \n",
    "    to_read=False\n",
    "    all_energies = None\n",
    "    with open(energies_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if to_read:\n",
    "                all_energies = line.split('|')\n",
    "                all_energies = [x.strip(' ') for x in all_energies]\n",
    "                all_energies = all_energies[5:18]\n",
    "                all_energies = [float(x) for x in all_energies]\n",
    "                all_energies = np.array(all_energies)\n",
    "                break\n",
    "            if 'Sol # |' in line:\n",
    "                to_read=True\n",
    "    if all_energies is not None:\n",
    "        all_energies = np.nan_to_num(all_energies)\n",
    "    elif assign_zeros:\n",
    "        all_energies = np.zeros(13)\n",
    "        \n",
    "    return all_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T03:54:02.627375500Z",
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read energies from all positive complexes\n",
    "\n",
    "all_energies_list = []\n",
    "for ppi in train_list:\n",
    "    energy_path = f\"{config['dirs']['grid']}/refined-out-{ppi}.ref\"\n",
    "    if os.path.exists(energy_path):\n",
    "        energy_i = read_energies(energy_path)\n",
    "        if energy_i is not None:\n",
    "            all_energies_list.append(energy_i)\n",
    "\n",
    "print(f\"Loaded energy terms from {len(all_energies_list)} native complexes\")\n",
    "all_energies = np.stack(all_energies_list, axis=0)\n",
    "all_energies_mean = np.mean(all_energies, axis=0)\n",
    "all_energies_std = np.std(all_energies, axis=0)\n",
    "print(f\"Energies mean: {list(all_energies_mean)}\")\n",
    "print(f\"Energies std: {list(all_energies_std)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply contrastive loss to the final embeddings;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from networks.ViT_pytorch import Encoder\n",
    "from networks.ViT_hybrid import ViT_Hybrid_encoder\n",
    "import pdb\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from losses.proto_loss import ProtoLoss\n",
    "from losses.supCon_loss import SupConLoss\n",
    "\n",
    "class PISToN_proto(nn.Module):\n",
    "\n",
    "    def __init__(self, config, img_size=24, num_classes=2, zero_head=False, margin=0, temperature=0.1):\n",
    "        super(PISToN_proto, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Input: Image with the following features:\n",
    "\n",
    "        0 - Shape index | p1\n",
    "        1 - Distance depended curvature | p1\n",
    "        2 - Hydrogen bond potential | p1\n",
    "        3 - Charge | p1\n",
    "        4 - Hydrophobicity | p1\n",
    "        5 - Shape index | p1\n",
    "        6 - Distance depended curvature | p1\n",
    "        7 - Hydrogen bond potential | p1\n",
    "        8 - Charge | p1\n",
    "        9 - Hydrophobicity | p1\n",
    "        10 - Distance between atoms | p1 and p2\n",
    "        11 - Relative ASA | p1\n",
    "        12 - Relative ASA | p1\n",
    "\n",
    "        Energies with the following features:\n",
    "        0  - glob      - global score of the candidate, which is linear combination of the terms described bellow. To rank the candidates, you should sort the rows by this column in ascending order.\n",
    "        1  - aVdW      - attractive van der Waals\n",
    "        2  - rVdW      - repulsive van der Waals\n",
    "        3  - ACE       - Atomic Contact Energy | desolvation (10.1006/jmbi.1996.0859)\n",
    "        4  - inside    - \"Insideness\" measure, which reflects the concavity of the interface.\n",
    "        5  - aElec     - short-range attractive electrostatic term\n",
    "        6  - rElec     - short-range repulsive electrostatic term\n",
    "        7  - laElec    - long-range attractive electrostatic term\n",
    "        8  - lrElec    - long-range repulsive electrostatic term\n",
    "        9  - hb        - hydrogen and disulfide bonding\n",
    "        10 - piS\t     - pi-stacking interactions\n",
    "        11 - catpiS\t  - cation-pi interactions\n",
    "        12 - aliph\t  - aliphatic interactions\n",
    "\n",
    "        Loss = alpha * (BCE_i/5) + Global_BCE, where alpha is a hyperparameter from 0 to 1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # First tuple is the feature index in the image, and the second tuple is the feature index of energy terms\n",
    "        self.index_dict = {\n",
    "            'shape_complementarity': ( (0, 5, 1, 6, 10), (1,2,4) ),\n",
    "            'RASA': ( (11, 12, 10), (3,) ),\n",
    "            'hydrogen_bonds': ( (2, 7, 10), (9,) ),\n",
    "            'charge': ( (3, 8, 10), (5, 6, 7, 8, 10, 11, 12) ),\n",
    "            'hydrophobicity': ((4, 9, 10), ()),\n",
    "        }\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "\n",
    "\n",
    "        self.spatial_transformers_list = nn.ModuleList()\n",
    "\n",
    "        for feature in self.index_dict.keys():\n",
    "            self.spatial_transformers_list.append(self.init_transformer(config, channels=len(self.index_dict[feature][0]),\n",
    "                                                                n_individual=len(self.index_dict[feature][1])))\n",
    "\n",
    "\n",
    "        self.classifier = config.classifier\n",
    "        self.feature_transformer = Encoder(config, vis=True)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size), requires_grad=True)\n",
    "        self.proto_vector_pos = nn.Parameter(torch.rand(1,config.hidden_size), requires_grad=True)\n",
    "        self.proto_vector_neg = nn.Parameter(torch.rand(1,config.hidden_size), requires_grad=True)\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        #self.cosine_loss = CosineEmbeddingLoss(margin=margin)\n",
    "\n",
    "        # self.proto_loss_fn = ProtoLoss(margin=margin, prototype_activation_function=prototype_activation_function)\n",
    "\n",
    "        #self.final_head = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def init_transformer(self, config, channels, n_individual):\n",
    "        \"\"\"\n",
    "        Initialize Transformer Network for a given tupe of features\n",
    "        :param model_config:\n",
    "        :param channels:\n",
    "        :param n_individual:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return ViT_Hybrid_encoder(config, n_individual, img_size=self.img_size,\n",
    "                        num_classes=self.num_classes, channels=channels, vis=True)\n",
    "\n",
    "\n",
    "    def forward(self, img, energies, labels=None):\n",
    "\n",
    "        all_x = []\n",
    "        all_spatial_attn = []\n",
    "        for i, feature in enumerate(self.index_dict.keys()):\n",
    "            img_tmp = img[:,self.index_dict[feature][0],:,:]\n",
    "            energy_tmp = energies[:, self.index_dict[feature][1]]\n",
    "\n",
    "            x, attn = self.spatial_transformers_list[i](img_tmp, energy_tmp)\n",
    "\n",
    "            all_x.append(x)\n",
    "            all_spatial_attn.append(attn)\n",
    "\n",
    "        x = torch.stack(all_x, dim=1) # (36, 5, 16)\n",
    "\n",
    "        # Create classification token\n",
    "        B = x.shape[0] #batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x, feature_attn = self.feature_transformer(x)\n",
    "        # 这里只保留了每一个张量的第一个\n",
    "        x = x[:, 0]\n",
    "        x = nn.functional.normalize(x)  # L2 normalization\n",
    "\n",
    "        proto_vector_pos = nn.functional.normalize(self.proto_vector_pos)\n",
    "        proto_vector_neg = nn.functional.normalize(self.proto_vector_neg)\n",
    "\n",
    "        dist = nn.PairwiseDistance()\n",
    "        dist_to_pos_prototype = dist(x, proto_vector_pos.repeat(x.shape[0], 1))\n",
    "        dist_to_neg_prototype = dist(x, proto_vector_neg.repeat(x.shape[0], 1))\n",
    "\n",
    "        # 0 - minimize distance to positives\n",
    "        logits = torch.stack([dist_to_pos_prototype-dist_to_neg_prototype, dist_to_neg_prototype-dist_to_pos_prototype], axis=1)\n",
    "\n",
    "        #scores = dist_to_pos_prototype-dist_to_neg_prototype\n",
    "        scores = dist_to_pos_prototype\n",
    "        #print(labels)\n",
    "        #print(scores)\n",
    "        # print(dist_to_pos_prototype)\n",
    "        # print(logits)\n",
    "        # print(dist_to_pos_prototype)\n",
    "        # print()\n",
    "        # print(dist_to_neg_prototype)\n",
    "        # print()\n",
    "        if labels is not None:\n",
    "            proto_loss_fn = ProtoLoss(margin=self.margin)\n",
    "            bce_loss_fn = nn.CrossEntropyLoss()\n",
    "            SupConLoss_fn = SupConLoss(temperature=self.temperature, base_temperature=self.temperature*10)\n",
    "\n",
    "            proto_loss = proto_loss_fn(x, proto_vector_pos, proto_vector_neg, labels)\n",
    "            BCE_loss = bce_loss_fn(logits, labels) # probability of the positive class\n",
    "            supCon_loss = SupConLoss_fn(x, labels)\n",
    "            loss = proto_loss + BCE_loss + supCon_loss\n",
    "\n",
    "        # Do the Supervised Contrastive Loss jointly with Cosine Loss\n",
    "        #x = self.final_head(x)\n",
    "\n",
    "            return scores, (all_spatial_attn, feature_attn), loss\n",
    "        else:\n",
    "            return scores, (all_spatial_attn, feature_attn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "from networks.ViT_pytorch import get_ml_config\n",
    "from utils.trainer import evaluate_val, fit_supCon\n",
    "\n",
    "def train_piston(search_space, train_list, val_list, SEED_ID, IMG_SIZE, PATIENCE, MODEL_DIR,\n",
    "                  MODEL_NAME, docked_dir, pos_grid_dir, \n",
    "                  std, mean, energies_std, energies_mean,\n",
    "                  MAX_EPOCHS=50, N_FEATURES=10,\n",
    "                  feature_subset=None, disable_tqdm=True, print_summary=False, data_prepare_dir='./data_preparation/'):\n",
    "\n",
    "    assert len(mean) == N_FEATURES\n",
    "    if feature_subset is not None:\n",
    "        assert len(mean) == len(feature_subset)\n",
    "\n",
    "    train_db = PDB_complex_training(train_list,\n",
    "                                  training_mode=True,\n",
    "                                  feature_subset=feature_subset,\n",
    "                                  data_prepare_dir=data_prepare_dir,\n",
    "                                  neg_pos_ratio=search_space['neg_pos_ratio'],\n",
    "                                  mean=mean,\n",
    "                                  std=std,\n",
    "                                  energies_mean=energies_mean, \n",
    "                                  energies_std=energies_std)\n",
    "\n",
    "    val_db = PDB_complex_training(val_list,\n",
    "                                training_mode=False,\n",
    "                                feature_subset=feature_subset,\n",
    "                                data_prepare_dir=data_prepare_dir,\n",
    "                                neg_pos_ratio=search_space['neg_pos_ratio'],\n",
    "                                std=std,\n",
    "                                mean=mean,\n",
    "                                energies_mean=energies_mean, \n",
    "                                energies_std=energies_std)\n",
    "    \n",
    "    ##### Initialize data loaders\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(SEED_ID + worker_id)\n",
    "\n",
    "    trainloader = DataLoader(train_db, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = DataLoader(val_db, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ##### Initailize model\n",
    "    model_config=get_ml_config(search_space)\n",
    "    model = PISToN_proto(model_config, img_size=IMG_SIZE, temperature=search_space['temperature'],\n",
    "                        num_classes=2, margin=search_space['margin']).float()\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    ##### Load model if exists\n",
    "    if os.path.exists(MODEL_DIR + '/{}.pth'.format(MODEL_NAME)):\n",
    "        print(\"Resuming previously saved checkpoint...\")\n",
    "        model.load_state_dict(torch.load(MODEL_DIR + '/{}.pth'.format(MODEL_NAME)))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=search_space['lr'], weight_decay=search_space['weight_decay'])\n",
    "\n",
    "    ##### Fit function\n",
    "    model, history, saved_index = fit_supCon(MAX_EPOCHS, model, trainloader, valloader, optimizer,\n",
    "                                      model_name=MODEL_NAME, image_size=IMG_SIZE,\n",
    "                                      channels=N_FEATURES, device=device, save_model=True,\n",
    "                                      saved_model_dir=MODEL_DIR, patience=PATIENCE,\n",
    "                                      print_summary=print_summary, disable_tqdm=disable_tqdm, \n",
    "                                      include_energy=True, include_attn=True, inside_loss=True,\n",
    "                                     n_individual=len(energies_mean))\n",
    "    \n",
    "    val_loss, val_acc, val_auc, tn, fp, fn, tp = evaluate_val(valloader, model, device, include_energy=True, include_attn=True)\n",
    "\n",
    "    return model, history, saved_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "params = {'dim_head': DIM,\n",
    "          'hidden_size': DIM,\n",
    "          'dropout': 0,\n",
    "          'attn_dropout': 0,\n",
    "          'lr': 0.0001,\n",
    "          'n_heads': 8,\n",
    "          'neg_pos_ratio': 5,\n",
    "          'patch_size': 4,\n",
    "          'transformer_depth': 8,\n",
    "          'weight_decay': 0.0001,\n",
    "           'margin': MARGIN,\n",
    "             'temperature': TEMP}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model, history, saved_index = train_piston(params,\n",
    "                            train_list=train_list,\n",
    "                            val_list=val_list,\n",
    "                             SEED_ID=SEED_ID,\n",
    "                             IMG_SIZE=IMG_SIZE,\n",
    "                             PATIENCE=PATIENCE,\n",
    "                            MODEL_NAME=MODEL_NAME,\n",
    "                             MAX_EPOCHS=MAX_EPOCH,\n",
    "                             N_FEATURES=N_FEATURES,\n",
    "                             MODEL_DIR=MODEL_DIR,\n",
    "                            docked_dir=config['dirs']['docked'],\n",
    "                            pos_grid_dir=config['dirs']['grid'],\n",
    "                             mean=mean_array,\n",
    "                            std=std_array,\n",
    "                            energies_mean=all_energies_mean, \n",
    "                            energies_std=all_energies_std,\n",
    "                            feature_subset=FEATURES_SUBSET,\n",
    "                            disable_tqdm=False,\n",
    "                            print_summary=False,\n",
    "                            data_prepare_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply contrastive loss to the final embeddings;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from networks.ViT_pytorch import Encoder\n",
    "from networks.ViT_hybrid import ViT_Hybrid_encoder\n",
    "import pdb\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from losses.proto_loss import ProtoLoss\n",
    "from losses.supCon_loss import SupConLoss\n",
    "\n",
    "class PISToN_proto(nn.Module):\n",
    "\n",
    "    def __init__(self, config, img_size=24, num_classes=2, zero_head=False, margin=0, temperature=0.1):\n",
    "        super(PISToN_proto, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Input: Image with the following features:\n",
    "\n",
    "        0 - Shape index | p1\n",
    "        1 - Distance depended curvature | p1\n",
    "        2 - Hydrogen bond potential | p1\n",
    "        3 - Charge | p1\n",
    "        4 - Hydrophobicity | p1\n",
    "        5 - Shape index | p1\n",
    "        6 - Distance depended curvature | p1\n",
    "        7 - Hydrogen bond potential | p1\n",
    "        8 - Charge | p1\n",
    "        9 - Hydrophobicity | p1\n",
    "        10 - Distance between atoms | p1 and p2\n",
    "        11 - Relative ASA | p1\n",
    "        12 - Relative ASA | p1\n",
    "\n",
    "        Energies with the following features:\n",
    "        0  - glob      - global score of the candidate, which is linear combination of the terms described bellow. To rank the candidates, you should sort the rows by this column in ascending order.\n",
    "        1  - aVdW      - attractive van der Waals\n",
    "        2  - rVdW      - repulsive van der Waals\n",
    "        3  - ACE       - Atomic Contact Energy | desolvation (10.1006/jmbi.1996.0859)\n",
    "        4  - inside    - \"Insideness\" measure, which reflects the concavity of the interface.\n",
    "        5  - aElec     - short-range attractive electrostatic term\n",
    "        6  - rElec     - short-range repulsive electrostatic term\n",
    "        7  - laElec    - long-range attractive electrostatic term\n",
    "        8  - lrElec    - long-range repulsive electrostatic term\n",
    "        9  - hb        - hydrogen and disulfide bonding\n",
    "        10 - piS\t     - pi-stacking interactions\n",
    "        11 - catpiS\t  - cation-pi interactions\n",
    "        12 - aliph\t  - aliphatic interactions\n",
    "\n",
    "        Loss = alpha * (BCE_i/5) + Global_BCE, where alpha is a hyperparameter from 0 to 1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # First tuple is the feature index in the image, and the second tuple is the feature index of energy terms\n",
    "        self.index_dict = {\n",
    "            'shape_complementarity': ( (0, 5, 1, 6, 10), (1,2,4) ),\n",
    "            'RASA': ( (11, 12, 10), (3,) ),\n",
    "            'hydrogen_bonds': ( (2, 7, 10), (9,) ),\n",
    "            'charge': ( (3, 8, 10), (5, 6, 7, 8, 10, 11, 12) ),\n",
    "            'hydrophobicity': ((4, 9, 10), ()),\n",
    "        }\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "\n",
    "\n",
    "        self.spatial_transformers_list = nn.ModuleList()\n",
    "\n",
    "        for feature in self.index_dict.keys():\n",
    "            self.spatial_transformers_list.append(self.init_transformer(config, channels=len(self.index_dict[feature][0]),\n",
    "                                                                n_individual=len(self.index_dict[feature][1])))\n",
    "\n",
    "\n",
    "        self.classifier = config.classifier\n",
    "        self.feature_transformer = Encoder(config, vis=True)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size), requires_grad=True)\n",
    "        self.proto_vector_pos = nn.Parameter(torch.rand(1,config.hidden_size), requires_grad=True)\n",
    "        self.proto_vector_neg = nn.Parameter(torch.rand(1,config.hidden_size), requires_grad=True)\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        #self.cosine_loss = CosineEmbeddingLoss(margin=margin)\n",
    "\n",
    "        # self.proto_loss_fn = ProtoLoss(margin=margin, prototype_activation_function=prototype_activation_function)\n",
    "\n",
    "        #self.final_head = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def init_transformer(self, config, channels, n_individual):\n",
    "        \"\"\"\n",
    "        Initialize Transformer Network for a given tupe of features\n",
    "        :param model_config:\n",
    "        :param channels:\n",
    "        :param n_individual:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return ViT_Hybrid_encoder(config, n_individual, img_size=self.img_size,\n",
    "                        num_classes=self.num_classes, channels=channels, vis=True)\n",
    "\n",
    "\n",
    "    def forward(self, img, energies, labels=None):\n",
    "\n",
    "        all_x = []\n",
    "        all_spatial_attn = []\n",
    "        for i, feature in enumerate(self.index_dict.keys()):\n",
    "            img_tmp = img[:,self.index_dict[feature][0],:,:]\n",
    "            energy_tmp = energies[:, self.index_dict[feature][1]]\n",
    "\n",
    "            x, attn = self.spatial_transformers_list[i](img_tmp, energy_tmp)\n",
    "\n",
    "            all_x.append(x)\n",
    "            all_spatial_attn.append(attn)\n",
    "\n",
    "        x = torch.stack(all_x, dim=1) # (36, 5, 16)\n",
    "\n",
    "        # Create classification token\n",
    "        B = x.shape[0] #batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x, feature_attn = self.feature_transformer(x)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = nn.functional.normalize(x)  # L2 normalization\n",
    "\n",
    "        proto_vector_pos = nn.functional.normalize(self.proto_vector_pos)\n",
    "        proto_vector_neg = nn.functional.normalize(self.proto_vector_neg)\n",
    "\n",
    "        dist = nn.PairwiseDistance()\n",
    "        dist_to_pos_prototype = dist(x, proto_vector_pos.repeat(x.shape[0], 1))\n",
    "        dist_to_neg_prototype = dist(x, proto_vector_neg.repeat(x.shape[0], 1))\n",
    "\n",
    "        # 0 - minimize distance to positives\n",
    "        logits = torch.stack([dist_to_pos_prototype-dist_to_neg_prototype, dist_to_neg_prototype-dist_to_pos_prototype], axis=1)\n",
    "\n",
    "        scores = dist_to_pos_prototype-dist_to_neg_prototype\n",
    "        #scores = dist_to_pos_prototype\n",
    "        #print(labels)\n",
    "        #print(scores)\n",
    "        # print(dist_to_pos_prototype)\n",
    "        # print(logits)\n",
    "        # print(dist_to_pos_prototype)\n",
    "        # print()\n",
    "        # print(dist_to_neg_prototype)\n",
    "        # print()\n",
    "        if labels is not None:\n",
    "            proto_loss_fn = ProtoLoss(margin=self.margin)\n",
    "            bce_loss_fn = nn.CrossEntropyLoss()\n",
    "            SupConLoss_fn = SupConLoss(temperature=self.temperature, base_temperature=self.temperature*10)\n",
    "\n",
    "            proto_loss = proto_loss_fn(x, proto_vector_pos, proto_vector_neg, labels)\n",
    "            BCE_loss = bce_loss_fn(logits, labels) # probability of the positive class\n",
    "            supCon_loss = SupConLoss_fn(x, labels)\n",
    "            loss = proto_loss + BCE_loss + supCon_loss\n",
    "\n",
    "        # Do the Supervised Contrastive Loss jointly with Cosine Loss\n",
    "        #x = self.final_head(x)\n",
    "\n",
    "            return scores, (all_spatial_attn, feature_attn), loss\n",
    "        else:\n",
    "            return scores, (all_spatial_attn, feature_attn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model=None\n",
    "model_config=get_ml_config(params)\n",
    "model = PISToN_proto(model_config, img_size=32,\n",
    "                        num_classes=2).float()\n",
    "model.load_state_dict(torch.load(MODEL_DIR + '/{}.pth'.format(MODEL_NAME)))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "masif_processed_pos_list = [x.strip('\\n') for x in open('../data/lists/processed_masif_pos.txt')]\n",
    "masif_processed_neg_list = [x.strip('\\n') for x in open('../data/lists/processed_masif_neg.txt')]\n",
    "\n",
    "GRID_DIR = \"../data/masif_test/prepare_energies_16R/07-grid/\"\n",
    "\n",
    "## Read grid images\n",
    "grid_pos = []\n",
    "grid_neg = []\n",
    "energies_pos = []\n",
    "energies_neg = []\n",
    "for ppi in masif_processed_pos_list:\n",
    "    grid = np.load(f\"{GRID_DIR}/{ppi}.npy\", allow_pickle=True)\n",
    "    grid_pos.append(grid)\n",
    "\n",
    "    energies_path = f\"{GRID_DIR}/refined-out-{ppi}.ref\"\n",
    "    energies = read_energies(energies_path, assign_zeros=True)\n",
    "    energies_pos.append(energies)\n",
    "\n",
    "for ppi in masif_processed_neg_list:\n",
    "    grid = np.load(f\"{GRID_DIR}/{ppi}.npy\", allow_pickle=True)\n",
    "    grid_neg.append(grid)\n",
    "\n",
    "    energies_path = f\"{GRID_DIR}/refined-out-{ppi}.ref\"\n",
    "    energies = read_energies(energies_path, assign_zeros=True)\n",
    "    energies_neg.append(energies)\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(grid_pos)} positive and {len(grid_neg)} negative testing complexes\")\n",
    "\n",
    "# Read grid\n",
    "\n",
    "grid = np.stack(grid_pos+grid_neg, axis=0)\n",
    "true_label = [1 for x in range(len(grid_pos))] + [0 for x in range(len(grid_neg))]\n",
    "# Place features to the 1st access\n",
    "grid = np.swapaxes(grid, -1, 1).astype(np.float32)\n",
    "grid = grid[:,FEATURES_SUBSET,:,:]\n",
    "grid.shape\n",
    "\n",
    "# Read energies\n",
    "all_energies = np.stack(energies_pos+energies_neg, axis=0)\n",
    "all_energies.shape\n",
    "\n",
    "\n",
    "#### Standard scaling\n",
    "## IMAGE\n",
    "\n",
    "def learn_background_mask(grid):\n",
    "    \"\"\"\n",
    "    Returns the mask with zero elements outside the patch\n",
    "    :param grid: example of a grid image\n",
    "    :return: mask\n",
    "    \"\"\"\n",
    "    mask = np.zeros((grid.shape[0], grid.shape[1]))\n",
    "    radius = grid.shape[0]/2\n",
    "    for row_i in range(grid.shape[0]):\n",
    "        for column_i in range(grid.shape[1]):\n",
    "            # Check if coordinates are within the radius\n",
    "            x = column_i - radius\n",
    "            y = radius - row_i\n",
    "            if x ** 2 + y ** 2 <= radius ** 2:\n",
    "                mask[row_i][column_i] = 1\n",
    "    return mask\n",
    "\n",
    "background_mask = learn_background_mask(grid_pos[0])\n",
    "\n",
    "for feature_i in range(grid.shape[1]):\n",
    "    grid[:,feature_i,:,:] = (grid[:,feature_i,:,:] - mean_array[feature_i])/std_array[feature_i]\n",
    "    # Mask out values that are out of the radius:\n",
    "    grid = np.logical_and(grid, background_mask)*grid\n",
    "    \n",
    "## ENERGIES\n",
    "for energy_i in range(all_energies.shape[1]):\n",
    "    all_energies[:,energy_i] = (all_energies[:,energy_i] - all_energies_mean[energy_i])/all_energies_std[energy_i]\n",
    "\n",
    "## Inference\n",
    "device=torch.device(\"cpu\")\n",
    "grid = torch.from_numpy(grid).to(device)\n",
    "all_energies = torch.from_numpy(all_energies).float().to(device)\n",
    "model = model.to(device)\n",
    "output, attn =model(grid, all_energies)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "pred_probabilities = output.cpu().detach().numpy()\n",
    "\n",
    "# Select probability of the complex to be positive (y=1)\n",
    "#pred_probabilities = [float(x[1]) for x in pred_probabilities]\n",
    "\n",
    "auc = metrics.roc_auc_score(true_label, pred_probabilities)\n",
    "print(f\"Final test AUC of {MODEL_NAME} with FireDock energy terms: {auc}\")\n",
    "\n",
    "import pandas as pd\n",
    "## Save the file with all scores\n",
    "all_ppis = masif_processed_pos_list + masif_processed_neg_list\n",
    "out_file = f\"{MODEL_DIR}/{MODEL_NAME}_scores.csv\"\n",
    "with open(out_file, 'w') as out:\n",
    "    out.write(\"PPI,score,label\\n\")\n",
    "    for i in range(len(pred_probabilities)):\n",
    "        out.write(f\"{all_ppis[i]},{pred_probabilities[i]},{true_label[i]}\\n\")\n",
    "transsbind_df = pd.read_csv(out_file)\n",
    "\n",
    "### other scores\n",
    "import pandas as pd\n",
    "\n",
    "SCORES_MASIF=\"../data/masif_test/MaSIF-Search_scores.csv\"\n",
    "OTHER_SCORES=\"../data/masif_test/Other_tools_SCORES.csv\"\n",
    "\n",
    "scores_masif = pd.read_csv(SCORES_MASIF)\n",
    "scores_other = pd.read_csv(OTHER_SCORES)\n",
    "\n",
    "def plot_ROC(df, score_name, label_name, model_name, ax, color, pos_label):\n",
    "    fpr, tpr, thresh = roc_curve(df[label_name], df[score_name], pos_label=pos_label)\n",
    "    if not pos_label:\n",
    "        labels = -df[label_name] + 1\n",
    "    else:\n",
    "        labels=df[label_name]\n",
    "    ROC = roc_auc_score(labels, df[score_name])\n",
    "    ax.plot(fpr, tpr, linestyle='-',color=color, label=f'{model_name} (AUC={int(ROC*10000)/100})')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "colors = matplotlib.cm.Set1(range(9))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_ROC(transsbind_df, 'score', 'label', 'PISToN', ax, colors[0], pos_label=0)\n",
    "plot_ROC(scores_masif, 'score', 'label', 'MaSIF', ax, colors[8], pos_label=0)\n",
    "\n",
    "other_labels = ['FIREDOCK', 'AP_PISA', 'CP_PIE', 'PYDOCK_TOT', 'ZRANK2', 'ROSETTADOCK', 'SIPPER']\n",
    "pos_labels = [0,0,1,0,0,0,1]\n",
    "colors_array = ['r', 'c', 'm', 'y', 'black', 'orange', 'tan']\n",
    "for i in range(len(other_labels)):\n",
    "    plot_ROC(scores_other, other_labels[i], 'Label', other_labels[i], ax, colors[i+1], pos_label=pos_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "# # title\n",
    "plt.title('AUC ROC curve')\n",
    "# x label\n",
    "plt.xlabel('False Positive Rate')\n",
    "# y label\n",
    "plt.ylabel('True Positive rate')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'out_AUC_{MODEL_NAME}.png',dpi=600)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "## SUCCESS RATE ##\n",
    "##################\n",
    "\n",
    "## Read HADDOCK models\n",
    "CAPRI_LIST_FILE = '../data/lists/capri_list_models.txt'\n",
    "CAPRI_DIR=os.getcwd()+'/../data/benchmark_deepRank/'\n",
    "\n",
    "config = {}\n",
    "\n",
    "DATA_DIR = os.getcwd()+'/../data/benchmark_deepRank/piston_prepare/'\n",
    "config['dirs'] = {}\n",
    "config['dirs']['data_prepare'] = DATA_DIR\n",
    "config['dirs']['grid'] = config['dirs']['data_prepare'] + '07-grid/'\n",
    "config['ppi_const'] = {}\n",
    "config['ppi_const']['patch_r'] = 16 # 16\n",
    "\n",
    "\n",
    "\n",
    "capri_list = [x.strip('\\n') for x in open(CAPRI_LIST_FILE, 'r').readlines()]\n",
    "test_list_updated = get_processed(capri_list, config)\n",
    "\n",
    "print(f\"{len(test_list_updated)}/{len(capri_list)} complexes were processed.\")\n",
    "unprocessed_complexes = set(capri_list) - set(test_list_updated)\n",
    "\n",
    "print(f\"Unprocessed complexes: {unprocessed_complexes}\")\n",
    "print(\"The PISToN score for those complexes will be assigned to 0.\")\n",
    "\n",
    "unique_pids = list(set([x.split('-')[0] for x in capri_list]))\n",
    "print(unique_pids)\n",
    "\n",
    "def read_score_dict(data_dir, score_file_affix, unique_pids, print_labels=False):\n",
    "    score_dict = {}\n",
    "    for pid in unique_pids:\n",
    "        score_file = f\"{data_dir}/{pid}/{pid}.{score_file_affix}\"\n",
    "        all_scores = []\n",
    "        for line in open(score_file).readlines():\n",
    "            curr_model, score = line.strip('\\n').split('\\t')\n",
    "            model_id = curr_model.split('_')[0] + '-' + curr_model.split('_')[1]\n",
    "            score = float(score)\n",
    "            score_dict[model_id] = score\n",
    "            all_scores.append(score)\n",
    "        if print_labels:\n",
    "            print(f\"Sample {pid}: {int(np.sum(all_scores))} positives out of {len(all_scores)} models\")\n",
    "    return score_dict\n",
    "            \n",
    "\n",
    "labels_dict = read_score_dict(CAPRI_DIR, 'label', unique_pids, print_labels=True) \n",
    "\n",
    "# assign integer label\n",
    "for key_i in labels_dict.keys():\n",
    "    labels_dict[key_i] = int(labels_dict[key_i])\n",
    "    \n",
    "# Create labels array\n",
    "labels = []\n",
    "for ppi in test_list_updated:\n",
    "    model_id = ppi.split('_')[0]\n",
    "    labels.append(labels_dict[model_id])\n",
    "    \n",
    "print(f\"Total {np.sum(labels)} positive complexes out of {len(labels)}\")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PISToN_dataset(Dataset):\n",
    "    def __init__(self, grid_dir, ppi_list):\n",
    "\n",
    "        ### Empirically learned mean and standard deviations:\n",
    "        mean_array = [0.06383528408485302, 0.043833505848899605, -0.08456032982438057, 0.007828608135306595,\n",
    "                      -0.06060602411612203, 0.06383528408485302, 0.043833505848899605, -0.08456032982438057,\n",
    "                      0.007828608135306595, -0.06060602411612203, 11.390402735801011, 0.1496338245579665,\n",
    "                      0.1496338245579665]\n",
    "        std_array = [0.4507792893174703, 0.14148081793902434, 0.16581325050002976, 0.28599861830017204,\n",
    "                     0.6102229371168204, 0.4507792893174703, 0.14148081793902434, 0.16581325050002976,\n",
    "                     0.28599861830017204, 0.6102229371168204, 7.265311558033949, 0.18003612950610695,\n",
    "                     0.18003612950610695]\n",
    "\n",
    "        all_energies_mean = [-193.1392953586498, -101.97838818565408, 264.2099535864983, -17.27086075949363,\n",
    "                             16.329959915611877, -102.78101054852341, 36.531006329113836, -27.1124789029536,\n",
    "                             16.632626582278455, -8.784924050632918, -6.206329113924051, -1.8290084388185655,\n",
    "                             -11.827215189873417]\n",
    "        all_energies_std = [309.23521244706757, 66.75799437657368, 9792.783784373369, 25.384427268309658,\n",
    "                            7.929941961525389, 94.05055841984323, 47.22518557457095, 24.392679889433445,\n",
    "                            17.57399925906454, 7.041949880295568, 6.99554122803362, 2.557571754303165,\n",
    "                            13.666329541281653]\n",
    "\n",
    "        all_grids = []\n",
    "        all_energies = []\n",
    "\n",
    "        for ppi in ppi_list:\n",
    "            grid = np.load(f\"{grid_dir}/{ppi}.npy\", allow_pickle=True)\n",
    "            all_grids.append(grid)\n",
    "            energies_path = f\"{grid_dir}/refined-out-{ppi}.ref\"\n",
    "            energies = read_energies(energies_path, assign_zeros=True)\n",
    "            all_energies.append(energies)\n",
    "\n",
    "        background_mask = learn_background_mask(grid)\n",
    "\n",
    "        grid = np.stack(all_grids, axis=0)\n",
    "        grid = np.swapaxes(grid, -1, 1).astype(np.float32)\n",
    "        all_energies = np.stack(all_energies, axis=0)\n",
    "\n",
    "        print(f\"Interaction maps shape: {grid.shape}\")\n",
    "        print(f\"All energies shape: {all_energies.shape}\")\n",
    "\n",
    "        ### Standard scaling\n",
    "\n",
    "        # Interactino maps:\n",
    "        for feature_i in range(grid.shape[1]):\n",
    "            grid[:, feature_i, :, :] = (grid[:, feature_i, :, :] - mean_array[feature_i]) / std_array[feature_i]\n",
    "            # Mask out values that are out of the radius:\n",
    "            grid = np.logical_and(grid, background_mask) * grid\n",
    "\n",
    "        ## ENERGIES:\n",
    "        for energy_i in range(all_energies.shape[1]):\n",
    "            all_energies[:, energy_i] = (all_energies[:, energy_i] - all_energies_mean[energy_i]) / all_energies_std[\n",
    "                energy_i]\n",
    "\n",
    "        self.grid = grid\n",
    "        self.all_energies = all_energies\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.grid.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.grid[idx], self.all_energies[idx]\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "capri_dataset = PISToN_dataset(config['dirs']['grid'], test_list_updated)\n",
    "\n",
    "capri_loader = DataLoader(capri_dataset, batch_size=128, shuffle=False, pin_memory=False)\n",
    "\n",
    "from tqdm import tqdm\n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "# Infer in batches\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for grid, all_energies in tqdm(capri_loader):\n",
    "        grid = grid.to(device)\n",
    "        all_energies = all_energies.float().to(device)\n",
    "        model = model.to(device)\n",
    "        output, attn = model(grid, all_energies)\n",
    "        all_outputs.append(output)\n",
    "output = torch.cat(all_outputs, axis=0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.626375700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "# pred_probabilities = F.softmax(output, dim=1)\n",
    "pred_probabilities = list(output.cpu().detach().numpy())\n",
    "\n",
    "# Select probability of the complex to be positive (y=1)\n",
    "#pred_probabilities = [float(x[1]) for x in output]\n",
    "\n",
    "# append unprocessed complexes\n",
    "pred_probabilities = pred_probabilities + [1 for x in list(unprocessed_complexes)]\n",
    "all_capri_complexes = test_list_updated + list(unprocessed_complexes)\n",
    "true_labels = labels + [labels_dict[x.split('_')[0]] for x in list(unprocessed_complexes)]\n",
    "\n",
    "auc = metrics.roc_auc_score(true_labels, pred_probabilities)\n",
    "print(f\"Final test AUC for CAPRI dataset of {MODEL_NAME}: {1-auc}\")\n",
    "\n",
    "# other scores\n",
    "\n",
    "dove_dict = read_score_dict(CAPRI_DIR, 'dove', unique_pids) \n",
    "deeprank_dict = read_score_dict(CAPRI_DIR, 'deeprank', unique_pids) \n",
    "haddockScore_dict = read_score_dict(CAPRI_DIR, 'haddockScore', unique_pids) \n",
    "iScore_dict = read_score_dict(CAPRI_DIR, 'iScore', unique_pids) \n",
    "\n",
    "def assign_score(score_dict, all_capri_complexes):\n",
    "    score = []\n",
    "    for ppi in all_capri_complexes:\n",
    "        model_id = ppi.split('_')[0]\n",
    "        score.append(score_dict[model_id])\n",
    "    return score\n",
    "        \n",
    "dove_scores = assign_score(dove_dict, all_capri_complexes)\n",
    "deeprank_scores = assign_score(deeprank_dict, all_capri_complexes)\n",
    "haddockScore_scores = assign_score(haddockScore_dict, all_capri_complexes)\n",
    "iScore_scores = assign_score(iScore_dict, all_capri_complexes)\n",
    "\n",
    "auc = metrics.roc_auc_score(true_labels, dove_scores)\n",
    "print(f\"DOVE AUC for CAPRI dataset: {auc}\")\n",
    "\n",
    "auc = metrics.roc_auc_score(true_labels, deeprank_scores)\n",
    "print(f\"DeepRank AUC for CAPRI dataset: {1-auc}\")\n",
    "\n",
    "auc = metrics.roc_auc_score(true_labels, haddockScore_scores)\n",
    "print(f\"HADDOCK AUC for CAPRI dataset: {1-auc}\")\n",
    "\n",
    "auc = metrics.roc_auc_score(true_labels, iScore_scores)\n",
    "print(f\"iSCORE AUC for CAPRI dataset: {1-auc}\")\n",
    "\n",
    "capri_dict = read_score_dict(CAPRI_DIR, 'capri', unique_pids) \n",
    "capri_labels = assign_score(capri_dict, all_capri_complexes)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = {\"PISToN\":pred_probabilities, \n",
    "      \"DOVE\": dove_scores, \n",
    "      \"DeepRank\": deeprank_scores, \n",
    "      \"iSCORE\": iScore_scores, \n",
    "      \"HADDOCK\": haddockScore_scores,\n",
    "      \"label\": true_labels,\n",
    "      \"capri_quality\": capri_labels,\n",
    "     \"model_name\": all_capri_complexes,\n",
    "     #\"target\": \n",
    "     }\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df['target'] = df['model_name'].apply(lambda x: x.split('-')[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.627375500Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ROC(df, score_name, label_name, model_name, ax, color, pos_label):\n",
    "    fpr, tpr, thresh = roc_curve(df[label_name], df[score_name], pos_label=pos_label)\n",
    "    if not pos_label:\n",
    "        labels = -df[label_name] + 1\n",
    "    else:\n",
    "        labels=df[label_name]\n",
    "    ROC = roc_auc_score(labels, df[score_name])\n",
    "    ax.plot(fpr, tpr, linestyle='-',color=color, label=f'{model_name} (AUC={int(ROC*10000)/100})')\n",
    "\n",
    "    # matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "colors = matplotlib.cm.Set1(range(9))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "other_labels = ['PISToN', 'iSCORE', 'DeepRank', 'HADDOCK', 'DOVE']\n",
    "pos_labels = [0,0,0,0,1]\n",
    "colors_array = ['r', 'c', 'm', 'y', 'black']\n",
    "for i in range(len(other_labels)):\n",
    "    plot_ROC(df, other_labels[i], 'label', other_labels[i], ax, colors[i], pos_label=pos_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "# # title\n",
    "plt.title('AUC ROC curve')\n",
    "# x label\n",
    "plt.xlabel('False Positive Rate')\n",
    "# y label\n",
    "plt.ylabel('True Positive rate')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'AUC_CAPRI_{MODEL_NAME}.png',dpi=600)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T03:54:02.637191500Z",
     "start_time": "2023-12-30T03:54:02.628375300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_targets = list(df['target'].unique())\n",
    "\n",
    "def compute_success_rate(method, ascending, ax, color):\n",
    "    hits_target = np.zeros(200)\n",
    "    for target in all_targets:\n",
    "        target_df = df[df['target']==target]\n",
    "        target_df = target_df.sort_values(by=method, ascending=ascending).reset_index(drop=True)\n",
    "        for i in range(1, 200):\n",
    "            if 1 in list(target_df[:i]['label']):\n",
    "                for j in range(i, 200):\n",
    "                    hits_target[j] += 1/len(all_targets)\n",
    "                break\n",
    "    ax.plot(range(200), hits_target, linestyle='-',color=color, label=f'{method}')\n",
    "    return hits_target\n",
    "\n",
    "colors = matplotlib.cm.Set1(range(9))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "compute_success_rate('PISToN', True, ax, colors[0])\n",
    "\n",
    "methods = ['HADDOCK', 'DeepRank', 'iSCORE', 'DOVE']\n",
    "\n",
    "for i,method in enumerate(methods):\n",
    "    compute_success_rate(method, True, ax, colors[i+1])\n",
    "            \n",
    " # # title\n",
    "plt.title('Success Rate on CAPRI-score set')\n",
    "# x label\n",
    "plt.xlabel('Top N')\n",
    "# y label\n",
    "plt.ylabel('Success Rate')\n",
    "\n",
    "plt.legend(loc='best')           \n",
    "plt.savefig(f'HIT_RATE_{MODEL_NAME}.png',dpi=600)\n",
    "\n",
    "\n",
    "def compute_success_rate(method, ascending):\n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top5=0\n",
    "    top10 = 0\n",
    "    top25 = 0\n",
    "    top100= 0\n",
    "    top200 = 0\n",
    "    for target in all_targets:\n",
    "        target_df = df[df['target']==target]\n",
    "        target_df = target_df.sort_values(by=method, ascending=ascending).reset_index(drop=True)\n",
    "        if 1 in list(target_df[:1]['label']):\n",
    "             top1+=1\n",
    "        if 1 in list(target_df[:3]['label']):\n",
    "             top3+=1\n",
    "        if 1 in list(target_df[:5]['label']):\n",
    "             top5+=1\n",
    "        if 1 in list(target_df[:10]['label']):\n",
    "            top10+=1\n",
    "        if 1 in list(target_df[:25]['label']):\n",
    "            top25+=1\n",
    "        if 1 in list(target_df[:100]['label']):\n",
    "             top100+=1\n",
    "        if 1 in list(target_df[:200]['label']):\n",
    "            top200+=1\n",
    "    all_success_rate = [top1,top3, top5, top10, top25, top100, top200]\n",
    "    #all_success_rate = [top10, top25, top200]\n",
    "\n",
    "    all_success_rate = [int(x*100/len(all_targets)) for x in all_success_rate]\n",
    "    print(method)\n",
    "    print(all_success_rate)\n",
    "    #print(f\"Top1: {all_success_rate[0]} Top10: {all_success_rate[1]}; Top25: {all_success_rate[[2]]}; Top100: {all_success_rate[3]}; Top200: {all_success_rate[4]}\")\n",
    "    return all_success_rate\n",
    "\n",
    "\n",
    "#compute_success_rate('PISToN', True)\n",
    "methods = ['PISToN', 'HADDOCK', 'DeepRank', 'iSCORE', 'DOVE']\n",
    "all_scores = []\n",
    "for method in methods:\n",
    "    all_scores.append([method]+compute_success_rate(method, True))\n",
    "\n",
    "df = pd.DataFrame(all_scores)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T03:54:02.628375300Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
